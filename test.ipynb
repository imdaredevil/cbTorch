{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from tokenizers import BPETokenizer\n",
    "from layers import Embedding, FC, Flatten, Residual\n",
    "import numpy as np\n",
    "from models import Sequential\n",
    "from activations import LeakyRelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 10/44 [00:00<00:00, 35454.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: reached end of vocab\n",
      "[b'I ', b'I a', b'm ', b'm C', b'ib', b'ibi', b'm g', b're', b'rea', b'reat']\n",
      "[[b'I a', b'm C', b'ibi'], [b'I a', b'm g', b'reat']]\n",
      "[[257, 259, 261], [257, 262, 265]]\n",
      "['I am Cibi', 'I am great']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing tokenizer\n",
    "tokenizer = BPETokenizer()\n",
    "\n",
    "data = [\"I am Cibi\", \"I am great\"]\n",
    "# print(tokenizer.tokenize(data))\n",
    "tokenizer.train(data, max_vocab_size = 300)\n",
    "# print(len(tokenizer.tokens))\n",
    "print(tokenizer.tokens[256:])\n",
    "print(tokenizer.tokenize(data))\n",
    "tokens = tokenizer.encode(data)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 512)\n"
     ]
    }
   ],
   "source": [
    "# testing embedding\n",
    "embedding = Embedding(num_embeddings= tokenizer.vocab_size, embedding_dim=512)\n",
    "embed_output = embedding.forward(np.array(tokens))\n",
    "print(embed_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing out a simple model\n",
    "model = Sequential(\n",
    "    Embedding(num_embeddings=tokenizer.vocab_size, embedding_dim=512),\n",
    "    Flatten(),\n",
    "    FC(input_dim = 512 * 3, output_dim= 64, name = \"fc1\"),\n",
    "    LeakyRelu(name = \"relu_1\"),\n",
    "    FC(input_dim = 64, output_dim= 2, name = \"fc2\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizers import Optimizer, Adam\n",
    "import tqdm\n",
    "from models import Module\n",
    "from loss import Loss, CrossEntropyWithLogits\n",
    "import time\n",
    "def train(x, y, model: Module, loss: Loss, optimizer: Optimizer, epoch):\n",
    "    pbar = tqdm.tqdm(range(epoch))\n",
    "    for _ in pbar:\n",
    "        y_hat = model.forward(x)\n",
    "        curr_loss = loss.forward(y_hat, y)\n",
    "        pbar.set_description(f\"Loss: {curr_loss}\")\n",
    "        dloss = loss.backward()\n",
    "        model.backward(dloss)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.337818744673323e-14: 100%|██████████| 100/100 [00:01<00:00, 77.22it/s]\n"
     ]
    }
   ],
   "source": [
    "x = np.repeat(tokens, 100, axis = 0)\n",
    "y = np.repeat([0, 1], 100, axis=0)\n",
    "optim =  Adam(model)\n",
    "loss = CrossEntropyWithLogits(True)\n",
    "\n",
    "train(x, y, model, loss, optim, 100) # check whether loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16.20569471, -15.94303399],\n",
       "       [-15.65128024,  16.12947364]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(np.array(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931962285060865: 100%|██████████| 100/100 [00:01<00:00, 69.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# add a residual connection\n",
    "model2 = Sequential(\n",
    "    Embedding(num_embeddings=tokenizer.vocab_size, embedding_dim=512),\n",
    "    Flatten(),\n",
    "    FC(input_dim = 512 * 3, output_dim= 64, name = \"fc1\"),\n",
    "    LeakyRelu(name = \"relu_1\"),\n",
    "    Residual(FC(input_dim = 64, output_dim = 64, name = \"fc3\")),\n",
    "    FC(input_dim = 64, output_dim= 2, name = \"fc2\"),\n",
    ")\n",
    "y = np.random.choice([0, 1], size = (200,))\n",
    "train(x, y, model2, loss, optim, 100) # check whether loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
